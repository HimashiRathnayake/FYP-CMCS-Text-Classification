{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train Language Adapter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HimashiRathnayake/CMCS-Text-Classification/blob/main/XLM-R/Train_Language_Adapter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YOqoVKFK5fB",
        "outputId": "bfe9126b-08db-445f-add7-b8d838c76179"
      },
      "source": [
        "!git clone https://github.com/Adapter-Hub/adapter-transformers.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'adapter-transformers'...\n",
            "remote: Enumerating objects: 70992, done.\u001b[K\n",
            "remote: Counting objects: 100% (15972/15972), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3472/3472), done.\u001b[K\n",
            "remote: Total 70992 (delta 11814), reused 15576 (delta 11593), pack-reused 55020\u001b[K\n",
            "Receiving objects: 100% (70992/70992), 45.06 MiB | 13.94 MiB/s, done.\n",
            "Resolving deltas: 100% (52513/52513), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reQQU0KtL2Pk",
        "outputId": "decdd59e-ad77-4d96-b904-10bd50f2a00f"
      },
      "source": [
        "%cd adapter-transformers/examples/language-modeling/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/adapter-transformers/examples/language-modeling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0vk4xW9MbgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ec74296-ef06-4091-f265-486358701b4d"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install -U adapter-transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 34.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 55.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 49.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.0-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 392 kB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 52.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 47.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.8)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 33.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, huggingface-hub, datasets\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 datasets-1.16.1 frozenlist-1.2.0 fsspec-2021.11.1 huggingface-hub-0.2.0 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.2\n",
            "Collecting adapter-transformers\n",
            "  Downloading adapter_transformers-2.2.0-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (3.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (0.2.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 42.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 40.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 37.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->adapter-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->adapter-transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->adapter-transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, adapter-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed adapter-transformers-2.2.0 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_fLR_UKMYbS"
      },
      "source": [
        "import datasets\n",
        "import transformers\n",
        "from transformers import AdapterConfig\n",
        "from google.colab import drive"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5XmBVDkSQRY",
        "outputId": "f01151b9-4440-4dc2-90f5-7f49ee86fd7f"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_nxZ0IHLVbo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c60a6e3b-6871-4aa1-c421-923404a1285b"
      },
      "source": [
        "!python run_mlm.py \\\n",
        "    --model_name_or_path xlm-roberta-base \\\n",
        "    --train_file /content/drive/Shareddrives/FYP-CodeStars/Dataset/dataset.txt \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --learning_rate 1e-4 \\\n",
        "    --num_train_epochs 5 \\\n",
        "    --validation_split_percentage 1 \\\n",
        "    --output_dir /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter \\\n",
        "    --max_seq_length 128 \\\n",
        "    --train_adapter \\\n",
        "    --line_by_line True \\\n",
        "    --adapter_config \"pfeiffer+inv\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/02/2021 06:13:13 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/02/2021 06:13:13 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/runs/Dec02_06-13-13_f1a321f5908f,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "output_dir=/content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "12/02/2021 06:13:14 - WARNING - datasets.builder - Using custom data configuration default-b4c53c9f3adee395\n",
            "12/02/2021 06:13:14 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-b4c53c9f3adee395/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
            "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-b4c53c9f3adee395/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "100% 1/1 [00:00<00:00, 2814.97it/s]\n",
            "12/02/2021 06:13:14 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "12/02/2021 06:13:16 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "100% 1/1 [00:00<00:00, 91.15it/s]\n",
            "12/02/2021 06:13:16 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "12/02/2021 06:13:16 - INFO - datasets.builder - Generating split train\n",
            "12/02/2021 06:13:16 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-b4c53c9f3adee395/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 218.91it/s]\n",
            "12/02/2021 06:13:17 - WARNING - datasets.builder - Using custom data configuration default-b4c53c9f3adee395\n",
            "12/02/2021 06:13:17 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "12/02/2021 06:13:17 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-b4c53c9f3adee395/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "12/02/2021 06:13:17 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-b4c53c9f3adee395/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
            "12/02/2021 06:13:17 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-b4c53c9f3adee395/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "12/02/2021 06:13:18 - WARNING - datasets.builder - Using custom data configuration default-b4c53c9f3adee395\n",
            "12/02/2021 06:13:18 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "12/02/2021 06:13:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-b4c53c9f3adee395/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "12/02/2021 06:13:18 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-b4c53c9f3adee395/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
            "12/02/2021 06:13:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-b4c53c9f3adee395/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "[INFO|file_utils.py:1664] 2021-12-02 06:13:18,896 >> https://huggingface.co/xlm-roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpb8981_m1\n",
            "Downloading: 100% 512/512 [00:00<00:00, 356kB/s]\n",
            "[INFO|file_utils.py:1668] 2021-12-02 06:13:19,645 >> storing https://huggingface.co/xlm-roberta-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n",
            "[INFO|file_utils.py:1676] 2021-12-02 06:13:19,646 >> creating metadata file for /root/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n",
            "[INFO|configuration_utils.py:584] 2021-12-02 06:13:19,646 >> loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n",
            "[INFO|configuration_utils.py:623] 2021-12-02 06:13:19,648 >> Model config XLMRobertaConfig {\n",
            "  \"adapters\": {\n",
            "    \"adapters\": {},\n",
            "    \"config_map\": {},\n",
            "    \"fusion_config_map\": {},\n",
            "    \"fusions\": {}\n",
            "  },\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-12-02 06:13:20,394 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:584] 2021-12-02 06:13:21,142 >> loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n",
            "[INFO|configuration_utils.py:623] 2021-12-02 06:13:21,144 >> Model config XLMRobertaConfig {\n",
            "  \"adapters\": {\n",
            "    \"adapters\": {},\n",
            "    \"config_map\": {},\n",
            "    \"fusion_config_map\": {},\n",
            "    \"fusions\": {}\n",
            "  },\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1664] 2021-12-02 06:13:22,649 >> https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpwctax0hs\n",
            "Downloading: 100% 4.83M/4.83M [00:01<00:00, 3.42MB/s]\n",
            "[INFO|file_utils.py:1668] 2021-12-02 06:13:24,926 >> storing https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model in cache at /root/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
            "[INFO|file_utils.py:1676] 2021-12-02 06:13:24,926 >> creating metadata file for /root/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
            "[INFO|file_utils.py:1664] 2021-12-02 06:13:25,674 >> https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgfahr5bw\n",
            "Downloading: 100% 8.68M/8.68M [00:01<00:00, 4.55MB/s]\n",
            "[INFO|file_utils.py:1668] 2021-12-02 06:13:28,498 >> storing https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
            "[INFO|file_utils.py:1676] 2021-12-02 06:13:28,498 >> creating metadata file for /root/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-12-02 06:13:30,744 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-12-02 06:13:30,744 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-12-02 06:13:30,744 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-12-02 06:13:30,744 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-12-02 06:13:30,744 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:584] 2021-12-02 06:13:31,492 >> loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n",
            "[INFO|configuration_utils.py:623] 2021-12-02 06:13:31,493 >> Model config XLMRobertaConfig {\n",
            "  \"adapters\": {\n",
            "    \"adapters\": {},\n",
            "    \"config_map\": {},\n",
            "    \"fusion_config_map\": {},\n",
            "    \"fusions\": {}\n",
            "  },\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1664] 2021-12-02 06:13:32,845 >> https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpshu4vn95\n",
            "Downloading: 100% 1.04G/1.04G [00:28<00:00, 39.6MB/s]\n",
            "[INFO|file_utils.py:1668] 2021-12-02 06:14:01,104 >> storing https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
            "[INFO|file_utils.py:1676] 2021-12-02 06:14:01,104 >> creating metadata file for /root/.cache/huggingface/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
            "[INFO|modeling_utils.py:1325] 2021-12-02 06:14:01,105 >> loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
            "[INFO|modeling_utils.py:1592] 2021-12-02 06:14:06,530 >> All model checkpoint weights were used when initializing XLMRobertaForMaskedLM.\n",
            "\n",
            "[INFO|modeling_utils.py:1601] 2021-12-02 06:14:06,530 >> All the weights of XLMRobertaForMaskedLM were initialized from the model checkpoint at xlm-roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForMaskedLM for predictions without further training.\n",
            "[INFO|configuration.py:264] 2021-12-02 06:14:06,569 >> Adding adapter 'mlm'.\n",
            "Running tokenizer on dataset line_by_line:   0% 0/14 [00:00<?, ?ba/s]12/02/2021 06:14:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-b4c53c9f3adee395/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-042f164837849562.arrow\n",
            "Running tokenizer on dataset line_by_line: 100% 14/14 [00:00<00:00, 20.20ba/s]\n",
            "Running tokenizer on dataset line_by_line:   0% 0/1 [00:00<?, ?ba/s]12/02/2021 06:14:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-b4c53c9f3adee395/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-eaeaddb0f97a8b24.arrow\n",
            "Running tokenizer on dataset line_by_line: 100% 1/1 [00:00<00:00, 98.98ba/s]\n",
            "[INFO|trainer.py:228] 2021-12-02 06:14:18,954 >> The following columns in the training set  don't have a corresponding argument in `XLMRobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:1171] 2021-12-02 06:14:18,965 >> ***** Running training *****\n",
            "[INFO|trainer.py:1172] 2021-12-02 06:14:18,965 >>   Num examples = 13384\n",
            "[INFO|trainer.py:1173] 2021-12-02 06:14:18,965 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1174] 2021-12-02 06:14:18,966 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1175] 2021-12-02 06:14:18,966 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1176] 2021-12-02 06:14:18,966 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1177] 2021-12-02 06:14:18,966 >>   Total optimization steps = 8365\n",
            "{'loss': 4.294, 'learning_rate': 9.402271368798566e-05, 'epoch': 0.3}\n",
            "  6% 500/8365 [02:28<42:54,  3.05it/s][INFO|trainer.py:134] 2021-12-02 06:16:47,606 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-500\n",
            "[INFO|loading.py:59] 2021-12-02 06:16:47,616 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-500/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:16:47,660 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-500/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:16:47,666 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:16:51,692 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-500/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:16:51,699 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:16:58,323 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-500/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:16:58,727 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:16:58,732 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 4.0377, 'learning_rate': 8.804542737597131e-05, 'epoch': 0.6}\n",
            " 12% 1000/8365 [05:12<42:48,  2.87it/s][INFO|trainer.py:134] 2021-12-02 06:19:31,338 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1000\n",
            "[INFO|loading.py:59] 2021-12-02 06:19:31,348 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1000/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:19:31,391 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1000/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:19:31,399 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:19:34,998 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1000/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:19:35,006 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:19:38,502 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1000/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:19:38,714 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:19:38,730 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 3.9712, 'learning_rate': 8.206814106395697e-05, 'epoch': 0.9}\n",
            " 18% 1500/8365 [07:56<29:26,  3.89it/s][INFO|trainer.py:134] 2021-12-02 06:22:15,811 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1500\n",
            "[INFO|loading.py:59] 2021-12-02 06:22:15,820 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1500/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:22:15,861 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1500/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:22:15,867 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:22:19,211 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1500/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:22:19,216 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:22:22,826 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1500/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:22:27,638 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:22:27,644 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 3.901, 'learning_rate': 7.609085475194262e-05, 'epoch': 1.2}\n",
            " 24% 2000/8365 [10:44<23:57,  4.43it/s][INFO|trainer.py:134] 2021-12-02 06:25:03,744 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2000\n",
            "[INFO|loading.py:59] 2021-12-02 06:25:03,754 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2000/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:25:03,793 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2000/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:25:03,809 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:25:07,399 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2000/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:25:07,412 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:25:11,094 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2000/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:25:11,417 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:25:11,422 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 3.7798, 'learning_rate': 7.011356843992828e-05, 'epoch': 1.49}\n",
            " 30% 2500/8365 [13:21<32:07,  3.04it/s][INFO|trainer.py:134] 2021-12-02 06:27:40,115 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2500\n",
            "[INFO|loading.py:59] 2021-12-02 06:27:40,127 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2500/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:27:40,180 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2500/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:27:40,188 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:27:44,250 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2500/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:27:44,257 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:27:47,686 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2500/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:27:47,923 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:27:47,928 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 3.7458, 'learning_rate': 6.413628212791392e-05, 'epoch': 1.79}\n",
            " 36% 3000/8365 [15:56<28:44,  3.11it/s][INFO|trainer.py:134] 2021-12-02 06:30:15,586 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3000\n",
            "[INFO|loading.py:59] 2021-12-02 06:30:15,594 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3000/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:30:15,631 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3000/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:30:15,637 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:30:18,997 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3000/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:30:19,095 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:30:22,595 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3000/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:30:23,098 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:30:23,103 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 3.7695, 'learning_rate': 5.8158995815899583e-05, 'epoch': 2.09}\n",
            " 42% 3500/8365 [18:32<21:04,  3.85it/s][INFO|trainer.py:134] 2021-12-02 06:32:51,665 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3500\n",
            "[INFO|loading.py:59] 2021-12-02 06:32:51,675 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3500/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:32:51,728 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3500/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:32:51,733 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:32:55,155 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3500/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:32:55,164 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:33:00,002 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3500/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:33:00,385 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:33:00,393 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 3.5911, 'learning_rate': 5.2181709503885237e-05, 'epoch': 2.39}\n",
            " 48% 4000/8365 [21:13<19:09,  3.80it/s][INFO|trainer.py:134] 2021-12-02 06:35:32,080 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4000\n",
            "[INFO|loading.py:59] 2021-12-02 06:35:32,090 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4000/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:35:32,142 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4000/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:35:32,150 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:35:35,734 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4000/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:35:35,743 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:35:39,243 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4000/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:35:39,790 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:35:43,915 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 3.565, 'learning_rate': 4.6204423191870897e-05, 'epoch': 2.69}\n",
            " 54% 4500/8365 [23:55<17:29,  3.68it/s][INFO|trainer.py:134] 2021-12-02 06:38:14,533 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4500\n",
            "[INFO|loading.py:59] 2021-12-02 06:38:14,550 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4500/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:38:14,590 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4500/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:38:14,598 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:38:18,281 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4500/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:38:18,782 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:38:22,612 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4500/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:38:22,619 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:38:26,578 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-4500/special_tokens_map.json\n",
            "{'loss': 3.72, 'learning_rate': 4.022713687985655e-05, 'epoch': 2.99}\n",
            " 60% 5000/8365 [26:36<16:14,  3.45it/s][INFO|trainer.py:134] 2021-12-02 06:40:55,597 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5000\n",
            "[INFO|loading.py:59] 2021-12-02 06:40:55,604 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5000/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:40:55,644 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5000/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:40:55,649 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:40:59,040 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5000/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:40:59,048 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:41:03,177 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5000/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:41:07,450 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:41:07,459 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5000/special_tokens_map.json\n",
            "{'loss': 3.5856, 'learning_rate': 3.42498505678422e-05, 'epoch': 3.29}\n",
            " 66% 5500/8365 [29:17<12:35,  3.79it/s][INFO|trainer.py:134] 2021-12-02 06:43:36,966 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5500\n",
            "[INFO|loading.py:59] 2021-12-02 06:43:36,977 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5500/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:43:37,026 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5500/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:43:37,031 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:43:40,429 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5500/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:43:40,477 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:43:44,609 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5500/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:43:44,615 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:43:48,901 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-5500/special_tokens_map.json\n",
            "{'loss': 3.5173, 'learning_rate': 2.8272564255827856e-05, 'epoch': 3.59}\n",
            " 72% 6000/8365 [32:01<12:33,  3.14it/s][INFO|trainer.py:134] 2021-12-02 06:46:20,403 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6000\n",
            "[INFO|loading.py:59] 2021-12-02 06:46:20,416 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6000/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:46:20,456 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6000/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:46:20,461 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:46:24,056 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6000/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:46:24,064 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:46:27,616 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6000/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:46:28,445 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:46:28,450 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6000/special_tokens_map.json\n",
            "{'loss': 3.4774, 'learning_rate': 2.229527794381351e-05, 'epoch': 3.89}\n",
            " 78% 6500/8365 [34:47<13:16,  2.34it/s][INFO|trainer.py:134] 2021-12-02 06:49:06,559 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6500\n",
            "[INFO|loading.py:59] 2021-12-02 06:49:06,566 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6500/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:49:06,623 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6500/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:49:06,633 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:49:09,978 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6500/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:49:09,988 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:49:13,869 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6500/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:49:17,561 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:49:17,566 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-6500/special_tokens_map.json\n",
            "{'loss': 3.5381, 'learning_rate': 1.6317991631799166e-05, 'epoch': 4.18}\n",
            " 84% 7000/8365 [37:29<07:00,  3.25it/s][INFO|trainer.py:134] 2021-12-02 06:51:48,378 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7000\n",
            "[INFO|loading.py:59] 2021-12-02 06:51:48,388 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7000/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:51:48,448 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7000/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:51:48,454 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:51:52,009 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7000/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:51:52,015 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:51:55,529 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7000/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:51:55,631 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:51:55,636 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7000/special_tokens_map.json\n",
            "{'loss': 3.4468, 'learning_rate': 1.0340705319784819e-05, 'epoch': 4.48}\n",
            " 90% 7500/8365 [40:08<03:33,  4.04it/s][INFO|trainer.py:134] 2021-12-02 06:54:27,358 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7500\n",
            "[INFO|loading.py:59] 2021-12-02 06:54:27,368 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7500/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:54:27,427 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7500/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:54:27,432 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:54:31,527 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7500/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:54:31,548 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7500/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:54:35,223 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7500/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:54:35,241 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:54:35,253 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-7500/special_tokens_map.json\n",
            "{'loss': 3.4054, 'learning_rate': 4.363419007770473e-06, 'epoch': 4.78}\n",
            " 96% 8000/8365 [42:48<01:55,  3.15it/s][INFO|trainer.py:134] 2021-12-02 06:57:07,010 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-8000\n",
            "[INFO|loading.py:59] 2021-12-02 06:57:07,028 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-8000/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:57:07,086 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-8000/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:57:07,091 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-8000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:57:10,775 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-8000/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:57:10,992 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-8000/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:57:14,491 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-8000/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:57:18,458 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-8000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:57:18,464 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/checkpoint-8000/special_tokens_map.json\n",
            "100% 8365/8365 [44:53<00:00,  3.12it/s][INFO|trainer.py:1384] 2021-12-02 06:59:12,701 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2693.7355, 'train_samples_per_second': 24.843, 'train_steps_per_second': 3.105, 'train_loss': 3.6960021352454424, 'epoch': 5.0}\n",
            "100% 8365/8365 [44:53<00:00,  3.11it/s]\n",
            "[INFO|trainer.py:134] 2021-12-02 06:59:12,721 >> Saving model checkpoint to /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter\n",
            "[INFO|loading.py:59] 2021-12-02 06:59:12,731 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/mlm/adapter_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:59:12,801 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:59:12,806 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:59:16,773 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/mlm/pytorch_model_head.bin\n",
            "[INFO|loading.py:59] 2021-12-02 06:59:16,779 >> Configuration saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/mlm/head_config.json\n",
            "[INFO|loading.py:72] 2021-12-02 06:59:20,272 >> Module weights saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/mlm/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-12-02 06:59:20,474 >> tokenizer config file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-12-02 06:59:20,637 >> Special tokens file saved in /content/drive/Shareddrives/FYP-CodeStars/Implementation/TrainedAdapters/language_adapter/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =      3.696\n",
            "  train_runtime            = 0:44:53.73\n",
            "  train_samples            =      13384\n",
            "  train_samples_per_second =     24.843\n",
            "  train_steps_per_second   =      3.105\n",
            "12/02/2021 06:59:24 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:228] 2021-12-02 06:59:24,664 >> The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:2246] 2021-12-02 06:59:24,678 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2248] 2021-12-02 06:59:24,679 >>   Num examples = 135\n",
            "[INFO|trainer.py:2251] 2021-12-02 06:59:24,679 >>   Batch size = 8\n",
            "100% 17/17 [00:02<00:00,  6.51it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_loss               =     2.8284\n",
            "  eval_runtime            = 0:00:02.61\n",
            "  eval_samples            =        135\n",
            "  eval_samples_per_second =     51.662\n",
            "  eval_steps_per_second   =      6.506\n",
            "  perplexity              =    16.9179\n",
            "[INFO|modelcard.py:446] 2021-12-02 06:59:28,156 >> Dropping the following result as it does not have all the necessary field:\n",
            "{'task': {'name': 'Masked Language Modeling', 'type': 'fill-mask'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swlxcJ9e9b8G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
